{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“spring2022_AI501_hw3.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **2022 Spring AI501 Homework assignment 3**\n",
        "Read the following problem sheet and submit your completed $\\texttt{ipynb}$ file."
      ],
      "metadata": {
        "id": "0x-CiiaUSb0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AltXgJLMSOH6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "In this problem, we empirically check the bias-variance tradeoff using a simple linear regression model. Let $p_\\text{data}(x, y)$ be a true data distribution defined as\n",
        "$$\n",
        "x \\sim \\mathrm{Unif}(0, 1), \\quad y | x \\sim \\mathcal{N}(\\sin(2\\pi x), \\sigma_y^2),\n",
        "$$\n",
        "where $\\sigma_y = 0.3$. During training, we don't have an access to this distribution but we are given a finite number of i.i.d. samples from it.\n",
        "$$\n",
        "\\mathcal{D}_\\text{tr} = (x_i, y_i)_{i=1}^n \\overset{\\mathrm{i.i.d.}}{\\sim} p_\\text{data}(x, y).\n",
        "$$\n",
        "The following codes implement the data generating process and show an example of training data generated from them."
      ],
      "metadata": {
        "id": "8IDTnJpgUDs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_true(x):\n",
        "    return np.sin(2*np.pi*x)\n",
        "\n",
        "def gen_data(n, sigma_y=0.3):\n",
        "    x = npr.rand(n)\n",
        "    y = f_true(x) + sigma_y*npr.randn(*x.shape)\n",
        "    return x, y\n",
        "\n",
        "npr.seed(42)\n",
        "x_tr, y_tr = gen_data(30)\n",
        "plt.plot(x_tr, y_tr, 'o')"
      ],
      "metadata": {
        "id": "RalfXDBVSkwl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5f83afb6-aad7-4923-9f81-b800cfb44e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff1a7518110>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given $\\mathcal{D}_\\text{tr}$, we setup a simple linear regression model with polynomial basis function of order $m$.\n",
        "$$\n",
        "f_\\theta(x) = \\theta^\\top \\phi(x), \\quad \\phi(x) = [1, x, \\dots, x^m]^\\top \\in \\mathbb{R}^{m+1}, \\quad \\theta \\in \\mathbb{R}^{m+1}.\n",
        "$$\n",
        "\n",
        "(a) Given the feature map $\\phi(x)$, we first compute the design matrix,\n",
        "$$\n",
        "\\Phi = \\begin{bmatrix} \\phi(x_1) & \\dots & \\phi(x_n)\\end{bmatrix}^\\top \\in \\mathbb{R}^{n \\times (m+1)}.\n",
        "$$\n",
        "Given $\\Phi$, we can compute the least square estimate of $\\theta$,\n",
        "$$\n",
        "\\mathcal{L}_\\text{LS}(\\mathcal{D}, \\theta) = \\frac{1}{2}\\sum_{(x,y)\\in \\mathcal{D}} \\Vert y - f_\\theta(x)\\Vert^2, \\quad \n",
        "\\hat\\theta = \\underset{\\theta}{\\mathrm{argmin}}\\,\\mathcal{L}_\\text{LS}(\\mathcal{D}_\\text{tr}, \\theta).\n",
        "$$\n",
        "Compute the following codes computing the design matrix and $\\hat\\theta$. You may use functions in $\\texttt{np.linalg}$ if needed."
      ],
      "metadata": {
        "id": "YPME3OJmWqSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct design matrix\n",
        "def get_Phi(x, m):    \n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "\n",
        "    ######################################################################\n",
        "\n",
        "# find least square solution\n",
        "def compute_theta_LS(x, y, m):    \n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "   \n",
        "    ######################################################################"
      ],
      "metadata": {
        "id": "92KI3F1NTwsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) We will compute the bias and variance of the estimators. As we learned from the class, the expected error of a generic estimator $f(\\cdot;\\mathcal{D})$ computed from an arbitrary dataset $\\mathcal{D}$ decomposes as\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathcal{D}) = \\underbrace{\\mathbb{E}_x[(\\mathbb{E}_{\\mathcal{D}}[f(x;\\mathcal{D})] - \\mathbb{E}_y[y|x])^2]}_{(\\text{bias})^2} + \\underbrace{\\mathbb{E}_x[\\mathbb{E}_{\\mathcal{D}}[(f(x;\\mathcal{D}) - \\mathbb{E}_{\\mathcal{D}}[f(x;\\mathcal{D})])^2]]}_{\\text{variance}} + \\sigma_y^2.\n",
        "$$\n",
        "\n",
        "We are trying to estimate $(\\text{bias})^2$ and $\\text{variance}$ with finite number of samples, that is, $\\mathcal{D}_1, \\mathcal{D}_2, \\dots, \\mathcal{D}_N \\overset{\\mathrm{i.i.d.}}{\\sim} p_\\text{data}$ with each $\\mathcal{D}_j$ contains $n$ data points. Assuming that we are given the values $\\{f(x_i;\\mathcal{D}_j)\\}_{i=1}^n$ for $j=1,\\dots, N$, complete the following code computing the empirical estimates of the $(\\text{bias})^2$ and $\\text{variance}$."
      ],
      "metadata": {
        "id": "qIxd1SmabF_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x: x values used to approximate the outer expectation E_x (test x)\n",
        "# fDs: matrix of size N * n, where N is the number of datasets and n is the number of data points per dataset.\n",
        "# The jth row of fDs is constructed from the jth dataset Dj. It contains the values of f(x;D_j) evaluated at x.\n",
        "\n",
        "def compute_bias2(x, fDs):\n",
        "    # fill this part\n",
        "    ######################################################################    \n",
        "\n",
        "    ######################################################################\n",
        "\n",
        "def compute_variance(x, fDs):\n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "\n",
        "    ######################################################################"
      ],
      "metadata": {
        "id": "3d3GmWEyMgh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Now let's compare the polynomial regression models with different orders. We will compare the models with various $m$ values using $N=300$ datasets by computing $(\\text{bias})^2$ and $\\text{variance}$. Fill in the blanks of the following codes."
      ],
      "metadata": {
        "id": "Eue1_jY3hT2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating datasets\n",
        "npr.seed(24)\n",
        "n = 25\n",
        "N = 300\n",
        "Ds = [gen_data(n) for _ in range(N)]\n",
        "n_test = 200\n",
        "x_test = np.sort(npr.rand(n_test))\n",
        "\n",
        "# Assess the least square estimate with the order m.\n",
        "# Returns the bias^2 and variance.\n",
        "def assess_LS_estimator(m, ax=None):\n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "    fDs = ...\n",
        "    ######################################################################\n",
        "\n",
        "    bias2 = compute_bias2(x_test, fDs)\n",
        "    variance = compute_variance(x_test, fDs)    \n",
        "\n",
        "    if ax is not None:\n",
        "        # plotting    \n",
        "        ax.set_title(fr'$m={m}$', fontsize=16)\n",
        "        for (x, y), fD in zip(Ds, fDs):\n",
        "            ax.plot(x, y, 'bo', alpha=0.1)\n",
        "            ax.plot(x_test, fD, 'r-', linewidth=0.7)    \n",
        "        ax.plot(x_test, fDs.mean(0), 'r-')\n",
        "        ax.set_ylim([-1.5, 1.5])\n",
        "\n",
        "    return bias2, variance\n",
        "\n",
        "# compare the models with m = 2, 3, ... 6.\n",
        "ms = np.arange(2, 7)\n",
        "fig, axes = plt.subplots(1, len(ms), figsize=(4*len(ms), 3))\n",
        "bias2s, variances = np.zeros(len(ms)), np.zeros(len(ms))\n",
        "for i, (m, ax) in enumerate(zip(ms, axes)):\n",
        "    bias2s[i], variances[i] = assess_LS_estimator(m, ax=ax)    \n",
        "plt.tight_layout()  \n",
        "\n",
        "plt.figure()\n",
        "plt.plot(ms, bias2s, 'ro-', label='(bias)^2')\n",
        "plt.plot(ms, variances, 'bx-', label='variance')\n",
        "plt.plot(ms, bias2s+variances, 'kd-', label='(bias)^2+variance')\n",
        "plt.xlabel(r'$m$', fontsize=16)\n",
        "plt.legend(fontsize=16)  "
      ],
      "metadata": {
        "id": "I8vS8veTh1f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Fix $m=9$. Consider a regularized least square estimator with loss function\n",
        "$$\n",
        "\\mathcal{L}_\\text{RLS}(\\mathcal{D},\\theta, \\lambda) = \\mathcal{L}_\\text{LS}(\\mathcal{D}, \\theta) + \\frac{\\lambda}{2}\\Vert\\theta\\Vert^2.\n",
        "$$\n",
        "Complete the following codes inspecting the bias-variance tradeoff of the regularized least square estimator with varying $\\lambda$ values."
      ],
      "metadata": {
        "id": "lUlXKMPuq1rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find regularized least square solution\n",
        "def compute_theta_RLS(x, y, m, lamb):    \n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "    \n",
        "    ######################################################################\n",
        "\n",
        "# Assess the regularized least square estimate with the order m and tradeoff parameter lamb.\n",
        "# Returns the bias^2 and variance\n",
        "def assess_RLS_estimator(m, log_lamb, ax=None):\n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "    fDs = ...\n",
        "    ######################################################################\n",
        "\n",
        "    bias2 = compute_bias2(x_test, fDs)\n",
        "    variance = compute_variance(x_test, fDs)    \n",
        "\n",
        "    if ax is not None:\n",
        "        # plotting    \n",
        "        ax.set_title(fr'$\\log\\,\\,\\lambda$={log_lamb:.3f}', fontsize=16)\n",
        "        for (x, y), fD in zip(Ds, fDs):\n",
        "            ax.plot(x, y, 'bo', alpha=0.1)\n",
        "            ax.plot(x_test, fD, 'r-', linewidth=0.7)    \n",
        "        ax.plot(x_test, fDs.mean(0), 'r-')\n",
        "        ax.set_ylim([-1.5, 1.5])\n",
        "\n",
        "    return bias2, variance\n",
        "\n",
        "# lambdas to compare\n",
        "log_lambs = np.linspace(-9.0, 0.0, 20)\n",
        "fig, axes = plt.subplots(1, 5, figsize=(4*5, 3))\n",
        "bias2s, variances = np.zeros(20), np.zeros(20)\n",
        "m = 9\n",
        "for i, log_lamb in enumerate(log_lambs):\n",
        "    ax = axes[i//4] if i%4 == 0 else None\n",
        "    bias2s[i], variances[i] = assess_RLS_estimator(m, log_lamb, ax=ax)        \n",
        "plt.tight_layout()  \n",
        "\n",
        "plt.figure()\n",
        "plt.plot(log_lambs, bias2s, 'ro-', label='(bias)^2')\n",
        "plt.plot(log_lambs, variances, 'bx-', label='variance')\n",
        "plt.plot(log_lambs, bias2s+variances, 'kd-', label='(bias)^2+variance')\n",
        "plt.xlabel(r'$\\log\\,\\,\\lambda$', fontsize=16)\n",
        "plt.legend(fontsize=16)  "
      ],
      "metadata": {
        "id": "i21pP8qEwWeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "In this problem, we will implement Principal component analysis (PCA) for image data. The following codes load, proproces, and visualize the MNIST handwritten digits dataset.\n"
      ],
      "metadata": {
        "id": "H3qVUrP42xdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "# preprocess data\n",
        "x = x_train.reshape((-1, 784)) / 255.0 \n",
        "# center data to have zero mean\n",
        "mean = x.mean(0)\n",
        "x = x - mean\n",
        "def plot_tiled_images(x, title=None):\n",
        "    fig, axes = plt.subplots(5, 5, figsize=(4, 4))        \n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "            img = (x[5*i+j] + mean).reshape((28, 28))\n",
        "            axes[i,j].imshow(img, cmap='gray')\n",
        "            axes[i,j].axis('off')    \n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    if title is not None:\n",
        "        plt.suptitle(title, fontsize=15)   \n",
        "plot_tiled_images(x, title='Original data')"
      ],
      "metadata": {
        "id": "Bs7uABAu7t6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Fill in the blanks of the following codes. The first function takes data $X \\in \\mathbb{R}^{n\\times d}$ and returns a projection matrix $W \\in \\mathbb{R}^{d\\times k}$ projecting $X$ onto $\\mathbb{R}^{k}$. The second function takes data $X$ and projection matrix $W$ to project and reconstruct the data $X' \\in \\mathbb{R}^{n\\times d}$. If PCA was done right, the reconstruction error between $X$ and $X'$ should be minimized."
      ],
      "metadata": {
        "id": "sOOX1_8Rw0NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computes a projection matrix to k dimensional subspace using PCA\n",
        "def PCA(x, k):\n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "   \n",
        "    ######################################################################\n",
        "\n",
        "def project_and_reconstruct(x, w):\n",
        "     # fill this part\n",
        "    ######################################################################\n",
        "   \n",
        "    ######################################################################\n",
        "\n",
        "ks = np.arange(10, 100, 20)\n",
        "w = PCA(x, ks[-1])\n",
        "for k in ks:    \n",
        "    plot_tiled_images(project_and_reconstruct(x, w[:,:k]), title=rf'Reconstruction with $k={k}$')   \n"
      ],
      "metadata": {
        "id": "U4kyZLTT-9tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Rather than choosing $k$ manually, we can choose $k$ in terms of the explained variance. Recall that the variance on the $j$th projected axis is given as $w_j^\\top S w_j$ where $w_j$ is the $j$th column of the projection matrix $W$ and $S$ is the empirical covariance of $X$. The explained variance of the projection matrix $W$ with $k$ columns is then defined as\n",
        "$$\n",
        "\\mathrm{EV}(k) = \\frac{\\sum_{j=1}^k w_j^\\top S w_j}{\\sum_{j=1}^d w_j^\\top S w_j}.\n",
        "$$\n",
        "Based on $\\mathrm{EV}(k)$, we can choose $k$ based on how much of the variance we would like to maintain in the projected space. For instance, if we want to project data so that 90% of the variance is explained, we can choose the smallest $k$ such that $\\mathrm{EV}(k) > 0.9$.\n",
        " \n",
        "Complete the following code implementing this scheme."
      ],
      "metadata": {
        "id": "5l8FroHayuxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA with k chosen to explain over thres portion of the variance\n",
        "def PCA_explained_variance(x, thres):\n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "          \n",
        "    ######################################################################\n",
        "\n",
        "thress = [0.8, 0.9, 0.95]\n",
        "for thres in thress:\n",
        "    w = PCA_explained_variance(x, thres)\n",
        "    plot_tiled_images(project_and_reconstruct(x, w),\n",
        "                      title=rf'{thres} of variance explained, $k={w.shape[1]}$') "
      ],
      "metadata": {
        "id": "2ksJbWxpshav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "In this problem, we will implement $k$-means clustering and apply it to a 2D clustering problem. \n",
        "The following code generates and plots the data to be used."
      ],
      "metadata": {
        "id": "l1S80b10L2OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets._samples_generator import make_blobs\n",
        "from matplotlib import cm\n",
        "x, y_true = make_blobs(n_samples=2000, centers=4, cluster_std=0.50, random_state=0)\n",
        "def plot_clustering(x, y):\n",
        "    uy = np.unique(y)    \n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(uy)))\n",
        "    for l, c in zip(uy, colors):\n",
        "        plt.scatter(x[y==l, 0], x[y==l,1], color=c, edgecolor=0.6*c)\n",
        "plot_clustering(x, y_true)\n"
      ],
      "metadata": {
        "id": "fSusB1Ji0IH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Complete the following code implementing $k$-means clustering algorithm. Try to minimize the use of $\\texttt{for}$ loop in the blank. Actually, you can complete it without any $\\texttt{for}$ loop if you use matrix and vector multiplications properly."
      ],
      "metadata": {
        "id": "f5E9MzCYMW7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes data x of shape (num_data, dimension), number of clusters k, and maximum number of iterations.\n",
        "# Returns the cluster labels z of shape (num_data,), the mean matrix of shape (k, dimension), \n",
        "# and the number of iterations run until the convergence.\n",
        "def kmeans(x, k, max_num_iter, init_means=None):\n",
        "    n = len(x)\n",
        "    z = np.zeros(n, dtype=int)\n",
        "\n",
        "    # randomly initialize centers\n",
        "    means = x[npr.permutation(n)[:k]].copy() if init_means is None else init_means\n",
        "\n",
        "    # loss for checking convergence\n",
        "    loss_prev = np.inf\n",
        "\n",
        "    for i in range(max_num_iter):\n",
        "        # fill this part\n",
        "        ######################################################################\n",
        "     \n",
        "        ######################################################################\n",
        "\n",
        "        loss = np.sum((x - means[z])**2)\n",
        "        if not np.isinf(loss_prev) and abs((loss_prev-loss)/loss_prev) < 1e-7:                    \n",
        "            break\n",
        "        else:\n",
        "            loss_prev = loss    \n",
        "\n",
        "    return z, means, i"
      ],
      "metadata": {
        "id": "czHVpWnw1ahr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have learned from the class, $k$-means clustering is quite sensitive to initialization. Run the following code block several times, and you will find that the algorithm occasionally fails and produces weird clustering results. Using the true label $\\texttt{y_true}$, we can check whether the results given by $k$-means are good or not. We will treat a clustering result $\\texttt{z}$ as a success if the adjusted Rand index (the score measuring similarity between two clustering labels) between $\\texttt{y_true}$ and $\\texttt{z}$ is larger than 0.95."
      ],
      "metadata": {
        "id": "iaYKpQXBM-j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "def check_success(z):\n",
        "    return adjusted_rand_score(z, y_true) > 0.95\n",
        "\n",
        "z, means, iter = kmeans(x, 4, 200)\n",
        "print(f'Converged at iter {iter}')\n",
        "if check_success(z):\n",
        "    print('Success')\n",
        "else:\n",
        "    print('Fail')\n",
        "plot_clustering(x, z)"
      ],
      "metadata": {
        "id": "jYMs5wpX4XJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code verifies the sensitivity of $k$-means to initializations by running it multiple times and counting the average success rate."
      ],
      "metadata": {
        "id": "Eh-JIPI-N1mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_sucess = 0\n",
        "avg_iter = 0\n",
        "num_trials = 500\n",
        "for _ in range(num_trials):\n",
        "    z, means, iter = kmeans(x, 4, 200)\n",
        "    avg_sucess += check_success(z)\n",
        "    avg_iter += iter\n",
        "print(f'success rate {avg_sucess/num_trials:.4f}, average iteration to converge {avg_iter/num_trials:.4f}')"
      ],
      "metadata": {
        "id": "JbUz_dAqBS15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Implement the $k$-means++ algorithm for initialization, and check if it improves the robustness of the algorithm. The pseudo-code of $k$-means++ can be found in the lecture note."
      ],
      "metadata": {
        "id": "a02I1XlOOC_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeanspp(x, k):\n",
        "    # fill this part\n",
        "    ######################################################################\n",
        "\n",
        "    ######################################################################\n",
        "\n",
        "avg_sucess = 0\n",
        "avg_iter = 0\n",
        "num_trials = 500\n",
        "for _ in range(num_trials):\n",
        "    z, means, iter = kmeans(x, 4, 200, init_means=kmeanspp(x, 4))\n",
        "    avg_sucess += check_success(z)\n",
        "    avg_iter += iter\n",
        "print(f'success rate {avg_sucess/num_trials:.4f}, average iteration to converge {avg_iter/num_trials:.4f}')"
      ],
      "metadata": {
        "id": "Cv5cYdfsCVif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1tyZE8ncx1JI"
      }
    }
  ]
}